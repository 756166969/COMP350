\documentclass [9 pt]{article}
\usepackage[margin = 1in]{geometry}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{arydshln}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{color}
\usepackage[dvipsnames]{xcolor}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{color}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{tikz}
\usepackage{booktabs}
\usepackage[framemethod = tikz]{mdframed}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}


\lstset{
  language=Matlab,
  basicstyle=\ttfamily,               
  numbers=left,                  
  stepnumber=1,                  
  numbersep=5pt,                  
  backgroundcolor=\color{white}, 
  showspaces=false,              
  showstringspaces=false,        
  showtabs=false,                
  tabsize=2,                     
  captionpos=b,                  
  breaklines=true,               
  breakatwhitespace=true,         
  title=\lstname,
  numberstyle=\tiny\color{Black},
  keywordstyle=\color{BrickRed},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},  
}

\theoremstyle{definition}
\newtheorem{problem}{Problem}
\newtheorem{theorem}{Theorem}
\newtheorem*{corollary}{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{conjecture}[theorem]{Conjecture}

\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}


\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{Yuhao Wu \quad 260711365} 
\rhead{\bfseries COMP 350 Final Review 4.2}
\cfoot{\thepage}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}


\setlength{\parindent}{0pt}


\begin{document}

\title{Polynomial Interpolation (25 points)}
\date{2018-11-30}
\author{Name: Yuhao Wu\\
ID Number: 260711365
}
\maketitle


\section*{Definition:}
A function $S$ is called a spline of degree $k$ if
\begin{itemize}
	\item The domain of $S$ is an interval $[a, b]$
	\item $S, S', S'', \ldots , S^{(k)}$ are continuous on $[a, b]$
	\item There are points $t_i$ (the knots of $S$) such that $a = t_0 < t_1 < \ldots < t_n = b$ and such that $S$ is a polynomial of degree at most $k$ on each $[t_i, t_{i+1}]$
\end{itemize}

\section*{Linear Spline:}
Given $n + 1$ points $(t_0, y_0), (t_1, y_1), \ldots, (t_n, y_n)$, obviously, we can write:
$$
S(n) = 
\begin{cases}
	S_0(x) \quad \quad t_0 \leq x \leq t_1\\
	S_1(x) \quad \quad t_1 \leq x \leq t_2\\
	\vdots\\
	S_{n - 1}(x) \quad\quad t_{n - 1} \leq x \leq t_n
\end{cases}
$$
where $S_i(x) = y_i + m_i \cdot (x - t_i), \quad m_i = \dfrac{y_{i + 1} - y_i}{t_{i+1} - t_i} \quad\quad t_i \leq x \leq t_{i+1}$
\\
\\
\textbf{Algorithm for evaluating $S(x)$}
\begin{algorithm}
\caption{Algorithm for evaluating $S(x)$}
\begin{algorithmic}[1]
\State Input: given $x, (t_i, y_i)\ and\ m_i$ \Comment{ $x$ is the point we want to evaluate }
\State \Comment{ $(t_i, y_i), i = 0, 1, \ldots ,n $  is the given knots}
\\
\For{ $k = 0 : n - 1$} \Comment{this is to find which interval  $x$ is in}
	\If{ $x - t_{i+1} \leq 0$}
		\State Exit Loop
	\EndIf
\EndFor
\\
\State $S\gets y_i + m_i\cdot (x - t_i)$

\end{algorithmic}
\end{algorithm}

In fact, if we choose to use \textit{ Binary Search } to find the exact interval, this will makes things more efficient. 

\newpage

\section*{Cubic Spline:}
\subsection*{Disadvantage of Linear Spline:}
For a Linear Spline, we generally don't have $S'$ to be continuous, so its graph lacks of smoothness.

\subsection*{Disadvantage of Quadratic Spline:}
For a Quadratic Spline, we generally don't have $S''$ to be continuous, so the curvature of its graph change abruptly at each \textbf{ knot }.
\\
\\
\\
So, the most frequently used splines are cubic spline.\\
\\
Given $n + 1$ points $(t_0, y_0), (t_1, y_1), \ldots, (t_n, y_n)$, obviously, we can write:
$$
S(n) = 
\begin{cases}
	S_0(x) \quad \quad t_0 \leq x \leq t_1\\
	S_1(x) \quad \quad t_1 \leq x \leq t_2\\
	\vdots\\
	S_{n - 1}(x) \quad\quad t_{n - 1} \leq x \leq t_n
\end{cases}
$$
where $S_i(x)$ is a cubic polynomial on $[t_i, t_{i+1}]$
\begin{itemize}
	\item \textbf{ Number Of Unknowns:}  Each $S_i$ has 4 unknowns. So, there are total $4n$ unknowns.
	\item \textbf{ Number Of Conditions:} $S(t_i) = y_i $ for $i = 0, 1, 2, \ldots n$, leads to $n + 1$ conditions.\\
	 $S_{i - 1}^{(k)}(t_i) = S_i^{(k)}(t_i)$ for $i = 0, 1, \ldots , n$ and $k = 0, 1 ,2$ gives us $3(n - 1)$ conditions.\\
	 Total $4n - 2$ conditions.
\end{itemize}

In order to get unique solution, we add two more extra conditions.
$$ S''(t_0) = S''(t_n) = 0 $$

\subsection*{EXAMPLE:}
\begin{center}
\begin{tabular}{ c|c c c } 
	x & -1 & 0 & 1\\
	\hline
	y & 1 & 2 & -1 
\end{tabular}
\end{center}
\begin{mdframed}
	$$
	S(x) = 
	\begin{cases}
		S_0(x) = a_0 + a_1\cdot x + a_2 \cdot x^2 + a_3 \cdot x^3\\
		\\
		S_1(x) = b_0 + b_1\cdot x + b_2 \cdot x^2 + b_3 \cdot x^3
	\end{cases}
	$$
	
	$$
	S'(x) = 
	\begin{cases}
		S'_0(x) = a_1 + 2  a_2 \cdot x + 3 a_3 \cdot x^2\\
		\\
		S'_1(x) = b_1 + 2b_2 \cdot x + 3b_3 \cdot x^2
	\end{cases}
	$$
	
	$$
	S''(x) = 
	\begin{cases}
		S''_0(x) = 2  a_2  + 6 a_3 \cdot x\\
		\\
		S''_1(x) = 2b_2  + 6b_3 \cdot x
	\end{cases}
	$$
	
	\begin{align*}
		S_0(-1) &= a_0 - a_1 + a_2 - a_3 = 1\\
		S_0(0) &= S_1(0) = 2 \implies a_0 = b_0 = 2\\
		S_1(1) &= b_0 + b_1 + b_2 + b_3 = -1\\
		&\quad\\
		S'_0(0)& = S'_1(0) \implies a_1 = b_1 \\ 
		&\quad\\
		S''_0(0) &= S''_1(0) \implies a_2 = b_2 \\
		&\quad\\
		S''_0(-1) &= 0 \implies 2a_2 = 6a_3\\
		S''_1(1) &= 0 \implies  2b_2 + 6b_3 = 0
	\end{align*}
	Thus, we have 
	$$ 
	\begin{cases}
		a_0 = 2 \quad a_1 = -1 \quad a_2 = -3 \quad a_3 = -1 \\
		\\
		b_0 = 2 \quad b_1 = -1 \quad b_2 = -3 \quad b_3 = 1 \\
	\end{cases} $$
	
\end{mdframed}




\section*{Least Square Approximation: }

\subsection*{Data Fitting By a Straight Line:}
Suppose the data are thought to conform to a linear relationship:
$$ y = a\cdot x + b$$
We want to solve the following optimization problem:
$$ \min_{a, b} \phi(a, b) \quad where\ \phi(a, b) = \sum_{k = 0}^m \Big( ax_k + b - y_k \Big)^2 $$
\\
From Calculus, the conditions that
$$ \dfrac{\partial\phi}{\partial a} = 0 \quad \quad \dfrac{\partial\phi}{\partial b} = 0 $$
are necessary at the minimum.
$$ \dfrac{\partial\phi}{\partial a} = 0 \implies 2 \cdot \sum_{k = 0}^m \Big( ax_k + b - y_k \Big) \cdot x_k = 0  \implies  a \cdot \sum_{k = 0}^m x_k^2 + b \cdot \sum_{k = 0}^m x_k = \sum_{k = 0}^m y_k \cdot x_k   $$
$$ \dfrac{\partial\phi}{\partial b} = 0 \implies 2 \cdot \sum_{k = 0}^m \Big( ax_k + b - y_k \Big)  = 0 \implies  a \cdot \sum_{k = 0}^m x_k + (m+1) \cdot b = \sum_{k = 0}^m y_k$$
which is to solve the linear equations as follows:
$$
\begin{bmatrix}
	\sum_{k = 0}^m x_k^2 & \cdot \sum_{k = 0}^m x_k\\
	\\
	\sum_{k = 0}^m x_k & (m+1)  
\end{bmatrix} \cdot
\begin{bmatrix}
	a\\
	\\
	b
\end{bmatrix} = 
\begin{bmatrix}
	\sum_{k = 0}^m y_k \cdot x_k \\
	\\
	 \sum_{k = 0}^m y_k
\end{bmatrix}
$$


\newpage
\subsection*{Data Fitting by a General Linear Family of Functions:}
Suppose that the data are thought to conform to a relationship like:
$$ y = \sum_{j = 0}^{n} c_j\cdot g_j(x) $$
where the functions $g_0, g_1, \ldots, g_n$ (called basis function) are known, the coefficients $c_0, c_1, \ldots ,c_n$ are to be determined.\\
\\
Then we need to solve the least square problem:
\begin{equation}
	\min_{c_0, c_1, \ldots, c_n} \phi(c_0, c_1, \ldots, c_n) \quad where\ \phi(c_0, c_1, \ldots, c_n) = \sum_{k = 0}^m \Big( \sum_{j = 0}^n c_j \cdot g_j(x_k) - y_k \Big)^2 
\end{equation}


Now, we calculate the first derivative:
$$ \dfrac{\partial \phi}{\partial c_i} = \sum_{k = 0}^m 2 \times \bigg[ \sum_{j = 0}^n c_j \cdot g_j(x_k) - y_k \bigg] \cdot g_i(x_k) = 0 \quad\quad i = 0, 1, 2, \ldots , n $$
Now, we have:

\begin{equation}
	\sum_{k = 0}^m  \bigg[ \sum_{j = 0}^n c_j \cdot g_j(x_k) - y_k \bigg] \cdot g_i(x_k) = 0  \implies \sum_{j = 0}^m  \bigg[ \sum_{k = 0}^n\cdot g_j(x_k)\cdot g_i(x_k)\bigg] \cdot c_j = \sum_{k = 0}^m g_i(x_k)\cdot y_k 
\end{equation}
\textbf{Normal Equations}

\subsection*{Another way of thinking:}
For Equation(1), we can easily write it into the form like:
$$ A = \begin{bmatrix}
	g_0(x_0) & g_1(x_0) & \ldots & g_n(x_0)\\
	g_0(x_0) & g_1(x_1) & \ldots & g_n(x_1)\\
	\vdots 	 & \vdots 	& \ldots & \vdots\\
	g_0(x_m) & g_1(x_m) & \ldots & g_n(x_m)
\end{bmatrix}  
\quad\quad
c = \begin{bmatrix}
	c_0\\
	c_1\\
	\vdots\\
	v_n
\end{bmatrix}
\quad\quad
y = \begin{bmatrix}
	y_0\\
	y_1\\
	\vdots\\
	y_m
\end{bmatrix}
$$
Then our problem changes into $$ \min_{c} || A \cdot c - y||_2^2$$
$A \cdot c$ can be considered as $A\cdot c = c_0 \cdot \textbf{a}_0 + c_1 \cdot \textbf{a}_1 + \ldots + c_n \cdot \textbf{a}_n$ where $\textbf{a}_i$ is the column vector of  matrix $A$.\\
We can assume that all the column vector of the $A$ are linearly independent, then this constructs a Hyper-Plane of dimension $m$.\\
\\
In order to minimize the $|| A\cdot c - y ||_2^2$, geometrically, we need $A\cdot c - y $ to be perpendicular to the Hyper-Plane, which means to perpendicular to every basis of vector space.  
\\
\\
Thus, we have: 
$$Ac - y \perp \textbf{a}_j \quad  j = 0, 1, \ldots ,n \implies \textbf{a}_j^T (Ac - y) = 0  \implies A^T (A\cdot c - y) = 0 \implies A^TA \cdot c = A^T \cdot y $$

\newpage
\textbf{If Basic Function is Exponential Function:}
$$y(x) = k \cdot e^{-\lambda\cdot x} $$
\begin{center}
\begin{tabular}{ c|c c c c } 
	$x$ & $x_0$ & $x_1$ & $\ldots$ & $x_m$\\
	\hline
	$y$ & $y_0$ & $y_1$ & $\ldots$ & $y_m$ 
\end{tabular}
\end{center}

Then we take $\ln$ both sides, which gives us $\ln y(x) = \ln k - \lambda \cdot x$\\
\\
Now, we just need to calculate
$$ \min \sum_{i = 0}^m (\ln k - \lambda \cdot x_i - \ln y_i )^2 $$
which is the same as straight line case.


\section*{EXAMPLE of Straight Line Data Fitting:}
TEXTBOOK PAGE 498 EXAMPLE:\\
\begin{center}
\begin{tabular}{ c|c c c c c } 
	$x$ & 4 & 7 & 11 & 13 & 17\\
	\hline
	$y$ & 2 & 0 & 2 & 6 & 7
\end{tabular}
\end{center}


\begin{mdframed}
	Straight Line can be considered as $y = ax + b$ where $g_0(x) = x\quad g_1(x) = b$\\
	\\
	Thus we can build matrix $A, c$ as following:
	$$A = \begin{bmatrix}
		4 & 1\\
		7 & 1\\
		11 & 1\\
		13 & 1\\
		17 & 1
	\end{bmatrix} \quad \quad 
	c = 
	\begin{bmatrix}
		a \\
		b
	\end{bmatrix}\quad\quad y = \begin{bmatrix}
		2\\
		0\\
		2\\
		6\\
		7
	\end{bmatrix}$$
	Now we want to find $c$ such that 
	$$ \min_{c} ||A\cdot c - y|| $$
	$$A^T \cdot A \cdot c = A^T \cdot y$$
	which gives us:
	$$\begin{bmatrix}
644 & 52 \\
52 & 5
\end{bmatrix} \cdot \begin{bmatrix}
	a\\
	b
\end{bmatrix}
 = 
 \begin{bmatrix}
 	227 \\
 	17
 \end{bmatrix}$$
\end{mdframed}













































\end{document}